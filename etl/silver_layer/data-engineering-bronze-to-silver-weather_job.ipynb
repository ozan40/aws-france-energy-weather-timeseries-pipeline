{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%timeout 20",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent timeout is None minutes.\ntimeout has been set to 20 minutes.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--job-bookmark-option\": \"job-bookmark-enable\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--job-bookmark-option': 'job-bookmark-enable'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.window import Window\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.job import Job\nimport pytz\nimport re\nimport unicodedata\n\n# Initialize all the variables needed\nsource_bucket = \"data-engineering-project-8433-3658-8863\"\nfolder_name = \"bronze_data\"\nprocessed_folder_name = \"silver_data\"\n\n# Set up catalog parameters\nglue_database = \"data-engineering-project-glue-database\"\nweather_table_name = \"raw_data_weather_data\"\nmapping_table_name = \"raw_data_french_region_city_mapping_20251116_210949_parquet\"\n\n# Set up the spark contexts, glue contexts and initialize job\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\ntry:\n    args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n    JOB_NAME = args['JOB_NAME']\nexcept:\n    JOB_NAME = \"notebook-job-weather-transform-final\"\n\njob.init(JOB_NAME, args if 'args' in locals() else {})",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": null,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nTimeout: 20\nSession ID: 9b6e4e8d-3387-44a8-8612-5fca732ba8a3\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--job-bookmark-option job-bookmark-enable\nWaiting for session 9b6e4e8d-3387-44a8-8612-5fca732ba8a3 to get into ready status...\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Helper Functions for Data Transformation\n# ============================================\n\ndef strip_accents(s):\n    if s is None:\n        return \"\"\n    return ''.join(c for c in unicodedata.normalize('NFKD', str(s)) \n                  if not unicodedata.combining(c))\n\ndef canonicalize(txt):\n    if txt is None or str(txt).lower() == 'nan':\n        return \"\"\n    s = strip_accents(str(txt).lower().strip())\n    for ch in [\"'\", \"’\", \"-\", \"_\", \".\", \",\", \"(\", \")\", \"/\", \"\\\\\"]:\n        s = s.replace(ch, \" \")\n    return \" \".join(s.split())\n\ndef normalize_city(s):\n    if s is None:\n        return \"\"\n    s = str(s).lower()\n    s = strip_accents(s)\n    s = re.sub(r\"[^a-z\\s\\-]\", \" \", s)\n    s = \" \".join(s.split())\n    s = s.replace(\" ste \", \" sainte \").replace(\" st \", \" saint \")\n    if s.startswith(\"ste \"): \n        s = \"sainte \" + s[4:]\n    if s.startswith(\"st \"):  \n        s = \"saint \" + s[3:]\n    return s\n\n# Region mapping configuration\nOLD_TO_NEW = {\n    \"aquitaine\": \"nouvelle aquitaine\", \n    \"poitou charentes\": \"nouvelle aquitaine\", \n    \"limousin\": \"nouvelle aquitaine\",\n    \"midi pyrenees\": \"occitanie\", \n    \"languedoc roussillon\": \"occitanie\",\n    \"burgundy\": \"bourgogne franche comte\", \n    \"franche comte\": \"bourgogne franche comte\",\n    \"alsace\": \"grand est\", \n    \"lorraine\": \"grand est\", \n    \"champagne ardenne\": \"grand est\",\n    \"haute normandie\": \"normandie\", \n    \"basse normandie\": \"normandie\",\n    \"nord pas de calais\": \"hauts de france\", \n    \"picardy\": \"hauts de france\", \n    \"picardie\": \"hauts de france\",\n    \"centre\": \"centre val de loire\", \n    \"brittany\": \"bretagne\",\n    \"rhone alpes\": \"auvergne rhone alpes\", \n    \"auvergne\": \"auvergne rhone alpes\",\n    \"paca\": \"provence alpes cote d azur\", \n    \"provence alpes\": \"provence alpes cote d azur\",\n    \"corsica\": \"corse\"\n}\n\nNEW_REGIONS = [\n    \"auvergne rhone alpes\", \"bourgogne franche comte\", \"bretagne\", \"centre val de loire\",\n    \"grand est\", \"hauts de france\", \"ile de france\", \"normandie\", \"nouvelle aquitaine\",\n    \"occitanie\", \"pays de la loire\", \"provence alpes cote d azur\", \"corse\"\n]\n\ndef map_region_to_new(name):\n    if name is None:\n        return None\n    c = canonicalize(name)\n    if c in OLD_TO_NEW:\n        return OLD_TO_NEW[c]\n    if c in NEW_REGIONS:\n        return c\n    if \"rhone alpe\" in c: \n        return \"auvergne rhone alpes\"\n    return c\n\ndef force_display_name_for_elec(raw):\n    if raw is None:\n        return None\n    if canonicalize(raw) in (\"paca\", \"provence alpes cote d azur\"):\n        return \"Provence-Alpes-Cote d'Azur\"\n    return str(raw)\n\n# Register UDFs\nstrip_accents_udf = udf(strip_accents, StringType())\ncanonicalize_udf = udf(canonicalize, StringType())\nnormalize_city_udf = udf(normalize_city, StringType())\nmap_region_to_new_udf = udf(map_region_to_new, StringType())\nforce_display_name_udf = udf(force_display_name_for_elec, StringType())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Read Source Data\n# ============================================\n\n# Read weather data from catalog\nweather_df_from_catalog = glueContext.create_data_frame_from_catalog(\n    glue_database,\n    weather_table_name,\n    additional_options={\"useCatalogSchema\": True, \"useSparkDataSource\": True, \"header\": True},\n    transformation_ctx=\"weather_df_from_catalog\"\n)\n\n# Read region-city mapping data from catalog\nmapping_df_from_catalog = glueContext.create_data_frame_from_catalog(\n    glue_database,\n    mapping_table_name,\n    additional_options={\"useCatalogSchema\": True, \"useSparkDataSource\": True, \"header\": True},\n    transformation_ctx=\"mapping_df_from_catalog\"\n)\n\nprint(\"=== SCHEMA INFORMATION ===\")\nprint(\"Weather data schema:\")\nweather_df_from_catalog.printSchema()\nprint(\"Weather columns:\", weather_df_from_catalog.columns)\nprint(\"Sample weather data:\")\nweather_df_from_catalog.show(5, truncate=False)\n\nprint(\"Mapping data schema:\")\nmapping_df_from_catalog.printSchema()\nprint(\"Sample mapping data:\")\nmapping_df_from_catalog.show(5, truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "=== SCHEMA INFORMATION ===\nWeather data schema:\nroot\n |-- city_name: string (nullable = true)\n |-- geoname_id: string (nullable = true)\n |-- date: long (nullable = true)\n |-- temperature_2m: float (nullable = true)\n |-- temperature_2m_previous_day1: float (nullable = true)\n |-- temperature_2m_previous_day2: float (nullable = true)\n |-- temperature_2m_previous_day3: float (nullable = true)\n |-- temperature_2m_previous_day4: float (nullable = true)\n |-- temperature_2m_previous_day5: float (nullable = true)\n |-- country_code: string (nullable = true)\n |-- population: long (nullable = true)\n |-- city_timezone: string (nullable = true)\n |-- lat: double (nullable = true)\n |-- lon: double (nullable = true)\n\nWeather columns: ['city_name', 'geoname_id', 'date', 'temperature_2m', 'temperature_2m_previous_day1', 'temperature_2m_previous_day2', 'temperature_2m_previous_day3', 'temperature_2m_previous_day4', 'temperature_2m_previous_day5', 'country_code', 'population', 'city_timezone', 'lat', 'lon']\nSample weather data:\n+---------------+----------+-------------------+--------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+------------+----------+-------------+--------+-------+\n|city_name      |geoname_id|date               |temperature_2m|temperature_2m_previous_day1|temperature_2m_previous_day2|temperature_2m_previous_day3|temperature_2m_previous_day4|temperature_2m_previous_day5|country_code|population|city_timezone|lat     |lon    |\n+---------------+----------+-------------------+--------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+------------+----------+-------------+--------+-------+\n|Marne La Vallée|12278193  |1753833600000000000|17.085        |17.035                      |16.0215                     |16.271502                   |16.3715                     |15.8654995                  |FR          |318325    |Europe/Paris |48.83584|2.64241|\n|Marne La Vallée|12278193  |1753837200000000000|16.935        |16.835                      |15.9215                     |16.2215                     |16.271502                   |15.7155                     |FR          |318325    |Europe/Paris |48.83584|2.64241|\n|Marne La Vallée|12278193  |1753840800000000000|16.835        |16.685                      |15.8715                     |16.2215                     |16.271502                   |15.565499                   |FR          |318325    |Europe/Paris |48.83584|2.64241|\n|Marne La Vallée|12278193  |1753844400000000000|16.935        |16.935                      |15.6715                     |16.2215                     |16.271502                   |15.5155                     |FR          |318325    |Europe/Paris |48.83584|2.64241|\n|Marne La Vallée|12278193  |1753848000000000000|16.885        |16.935                      |15.6715                     |16.1215                     |16.3215                     |15.4655                     |FR          |318325    |Europe/Paris |48.83584|2.64241|\n+---------------+----------+-------------------+--------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+------------+----------+-------------+--------+-------+\nonly showing top 5 rows\n\nMapping data schema:\nroot\n |-- region: string (nullable = true)\n |-- city: string (nullable = true)\n |-- scraped_at: timestamp (nullable = true)\n |-- source_url: string (nullable = true)\n |-- region_clean: string (nullable = true)\n |-- city_count: long (nullable = true)\n |-- city_first_letter: string (nullable = true)\n\nSample mapping data:\n+------+-----------+--------------------------+---------------------------------------------------------------------------+------------+----------+-----------------+\n|region|city       |scraped_at                |source_url                                                                 |region_clean|city_count|city_first_letter|\n+------+-----------+--------------------------+---------------------------------------------------------------------------+------------+----------+-----------------+\n|Alsace|Colmar     |2025-10-14 12:40:33.388156|https://www.britannica.com/topic/list-of-cities-and-towns-in-France-2039172|Alsace      |5         |C                |\n|Alsace|Haguenau   |2025-10-14 12:40:33.388156|https://www.britannica.com/topic/list-of-cities-and-towns-in-France-2039172|Alsace      |5         |H                |\n|Alsace|Mulhouse   |2025-10-14 12:40:33.388156|https://www.britannica.com/topic/list-of-cities-and-towns-in-France-2039172|Alsace      |5         |M                |\n|Alsace|Ribeauvillé|2025-10-14 12:40:33.388156|https://www.britannica.com/topic/list-of-cities-and-towns-in-France-2039172|Alsace      |5         |R                |\n|Alsace|Strasbourg |2025-10-14 12:40:33.388156|https://www.britannica.com/topic/list-of-cities-and-towns-in-France-2039172|Alsace      |5         |S                |\n+------+-----------+--------------------------+---------------------------------------------------------------------------+------------+----------+-----------------+\nonly showing top 5 rows\n\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Transform Region-City Mapping Data\n# ============================================\n\nmapping_df = mapping_df_from_catalog\n\nmapping_exploded = mapping_df.select(\n    col(\"region\").alias(\"region_raw\"),\n    col(\"city\").alias(\"city_raw\")\n).distinct()\n\nmapping_exploded = mapping_exploded.withColumn(\n    \"city_norm\", \n    normalize_city_udf(col(\"city_raw\"))\n).withColumn(\n    \"region_canon\",\n    map_region_to_new_udf(col(\"region_raw\"))\n).withColumn(\n    \"region_clean\",\n    force_display_name_udf(col(\"region_raw\"))\n).filter(\n    col(\"region_canon\").isin(NEW_REGIONS)\n)\n\nprint(\"Final transformed mapping data:\")\nmapping_exploded.show(20, truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Final transformed mapping data:\n+--------------------------+-----------+-----------+--------------------------+--------------------------+\n|region_raw                |city_raw   |city_norm  |region_canon              |region_clean              |\n+--------------------------+-----------+-----------+--------------------------+--------------------------+\n|Aquitaine                 |Pau        |pau        |nouvelle aquitaine        |Aquitaine                 |\n|Auvergne                  |Riom       |riom       |auvergne rhone alpes      |Auvergne                  |\n|Basse-Normandie           |Alençon    |alencon    |normandie                 |Basse-Normandie           |\n|Basse-Normandie           |Caen       |caen       |normandie                 |Basse-Normandie           |\n|Basse-Normandie           |Coutances  |coutances  |normandie                 |Basse-Normandie           |\n|Basse-Normandie           |Ouistreham |ouistreham |normandie                 |Basse-Normandie           |\n|Burgundy                  |Mâcon      |macon      |bourgogne franche comte   |Burgundy                  |\n|Haute-Normandie           |Dieppe     |dieppe     |normandie                 |Haute-Normandie           |\n|Île-de-France             |Créteil    |creteil    |ile de france             |Île-de-France             |\n|Île-de-France             |Montrouge  |montrouge  |ile de france             |Île-de-France             |\n|Île-de-France             |Provins    |provins    |ile de france             |Île-de-France             |\n|Languedoc-Roussillon      |Montpellier|montpellier|occitanie                 |Languedoc-Roussillon      |\n|Midi-Pyrenees             |Auch       |auch       |occitanie                 |Midi-Pyrenees             |\n|Nord-Pas-de-Calais        |Dunkirk    |dunkirk    |hauts de france           |Nord-Pas-de-Calais        |\n|Pays de la Loire          |Le Mans    |le mans    |pays de la loire          |Pays de la Loire          |\n|Picardy                   |Creil      |creil      |hauts de france           |Picardy                   |\n|Picardy                   |Senlis     |senlis     |hauts de france           |Picardy                   |\n|Provence-Alpes-Cote d’Azur|Martigues  |martigues  |provence alpes cote d azur|Provence-Alpes-Cote d'Azur|\n|Basse-Normandie           |Saint-Lô   |saint-lo   |normandie                 |Basse-Normandie           |\n|Brittany                  |Auray      |auray      |bretagne                  |Brittany                  |\n+--------------------------+-----------+-----------+--------------------------+--------------------------+\nonly showing top 20 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Transform and Join Weather Data with Regions\n# ============================================\n\nweather_df = weather_df_from_catalog\n\nprint(\"Starting weather data transformation...\")\n\n# Normalize city names in weather data\nweather_df = weather_df.withColumn(\n    \"city_norm\",\n    normalize_city_udf(col(\"city_name\"))\n)\n\nprint(\"After city normalization:\")\nweather_df.select(\"city_name\", \"city_norm\").show(10, truncate=False)\n\n# Handle numeric columns - only temperature columns are available\navailable_numeric_columns = []\npossible_numeric_columns = [\n    'temperature_2m', 'temperature_2m_previous_day1', 'temperature_2m_previous_day2', \n    'temperature_2m_previous_day3', 'temperature_2m_previous_day4', 'temperature_2m_previous_day5'\n]\n\nfor col_name in possible_numeric_columns:\n    if col_name in weather_df.columns:\n        available_numeric_columns.append(col_name)\n        weather_df = weather_df.withColumn(\n            col_name,\n            when(col(col_name).isNull(), lit(None))\n            .otherwise(\n                when(col(col_name).cast(\"double\").isNull(), lit(None))\n                .otherwise(col(col_name).cast(\"double\"))\n            )\n        )\n\nprint(f\"Available numeric columns: {available_numeric_columns}\")\n\n# Join weather data with region mapping\nweather_with_regions = weather_df.join(\n    mapping_exploded.select(\"city_norm\", \"region_clean\", \"region_canon\"),\n    on=\"city_norm\",\n    how=\"left\"\n)\n\nprint(f\"Weather data after region join: {weather_with_regions.count()} rows\")\n\n# Check mapping results\nmapped_count = weather_with_regions.filter(col(\"region_clean\").isNotNull()).count()\nunmapped_count = weather_with_regions.filter(col(\"region_clean\").isNull()).count()\nprint(f\"Mapped cities: {mapped_count}, Unmapped cities: {unmapped_count}\")\n\n# Show some unmapped cities for debugging\nif unmapped_count > 0:\n    print(\"Sample of unmapped cities:\")\n    weather_with_regions.filter(col(\"region_clean\").isNull()).select(\"city_name\", \"city_norm\").distinct().show(10, truncate=False)\n\n# Filter to only keep mapped cities\nweather_mapped = weather_with_regions.filter(col(\"region_clean\").isNotNull())\nprint(f\"Weather data after filtering mapped cities: {weather_mapped.count()} rows\")\n\n# Show which regions we have\nprint(\"Regions in mapped data:\")\nweather_mapped.select(\"region_clean\").distinct().show(truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 5,
			"outputs": [
				{
					"name": "stdout",
					"text": "Starting weather data transformation...\nAfter city normalization:\n+---------------+---------------+\n|city_name      |city_norm      |\n+---------------+---------------+\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n|Marne La Vallée|marne la vallee|\n+---------------+---------------+\nonly showing top 10 rows\n\nAvailable numeric columns: ['temperature_2m', 'temperature_2m_previous_day1', 'temperature_2m_previous_day2', 'temperature_2m_previous_day3', 'temperature_2m_previous_day4', 'temperature_2m_previous_day5']\nWeather data after region join: 498960 rows\nMapped cities: 439560, Unmapped cities: 59400\nSample of unmapped cities:\n+-------------------------+-------------------------+\n|city_name                |city_norm                |\n+-------------------------+-------------------------+\n|Marne La Vallée          |marne la vallee          |\n|Cergy-Pontoise           |cergy-pontoise           |\n|Boulogne-Billancourt     |boulogne-billancourt     |\n|Toulon                   |toulon                   |\n|Saint-Quentin-en-Yvelines|saint-quentin-en-yvelines|\n+-------------------------+-------------------------+\n\nWeather data after filtering mapped cities: 439560 rows\nRegions in mapped data:\n+--------------------------+\n|region_clean              |\n+--------------------------+\n|Rhône-Alpes               |\n|Brittany                  |\n|Provence-Alpes-Cote d'Azur|\n|Limousin                  |\n|Aquitaine                 |\n|Alsace                    |\n|Île-de-France             |\n|Auvergne                  |\n|Languedoc-Roussillon      |\n|Franche-Comté             |\n|Centre                    |\n|Champagne-Ardenne         |\n|Burgundy                  |\n|Nord-Pas-de-Calais        |\n|Basse-Normandie           |\n|Pays de la Loire          |\n|Lorraine                  |\n|Haute-Normandie           |\n|Picardy                   |\n|Midi-Pyrenees             |\n+--------------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Timestamp Processing\n# ============================================\n\nprint(\"Converting timestamp from nanoseconds...\")\n\nweather_mapped = weather_mapped.withColumn(\n    \"ts_utc\",\n    to_timestamp(from_unixtime(col(\"date\").cast(\"double\") / 1e9))\n).withColumn(\n    \"ts_paris\", \n    from_utc_timestamp(col(\"ts_utc\"), \"Europe/Paris\")\n)\n\nprint(\"After timestamp conversion:\")\nweather_mapped.select(\"date\", \"ts_utc\", \"ts_paris\").show(10, truncate=False)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Converting timestamp from nanoseconds...\nAfter timestamp conversion:\n+-------------------+-------------------+-------------------+\n|date               |ts_utc             |ts_paris           |\n+-------------------+-------------------+-------------------+\n|1753833600000000000|2025-07-30 00:00:00|2025-07-30 02:00:00|\n|1753837200000000000|2025-07-30 01:00:00|2025-07-30 03:00:00|\n|1753840800000000000|2025-07-30 02:00:00|2025-07-30 04:00:00|\n|1753844400000000000|2025-07-30 03:00:00|2025-07-30 05:00:00|\n|1753848000000000000|2025-07-30 04:00:00|2025-07-30 06:00:00|\n|1753851600000000000|2025-07-30 05:00:00|2025-07-30 07:00:00|\n|1753855200000000000|2025-07-30 06:00:00|2025-07-30 08:00:00|\n|1753858800000000000|2025-07-30 07:00:00|2025-07-30 09:00:00|\n|1753862400000000000|2025-07-30 08:00:00|2025-07-30 10:00:00|\n|1753866000000000000|2025-07-30 09:00:00|2025-07-30 11:00:00|\n+-------------------+-------------------+-------------------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Feature Engineering\n# ============================================\n\nprint(\"Creating weather features...\")\n\n# Use temperature_2m column\nweather_mapped = weather_mapped.withColumn(\"temp_c\", col(\"temperature_2m\"))\n\n# Calendar features\nweather_mapped = weather_mapped.withColumn(\n    \"date_paris\",\n    to_date(col(\"ts_paris\"))\n).withColumn(\n    \"hour\",\n    hour(col(\"ts_paris\"))\n).withColumn(\n    \"dow\",\n    dayofweek(col(\"ts_paris\"))\n).withColumn(\n    \"month\",\n    month(col(\"ts_paris\"))\n).withColumn(\n    \"doy\",\n    dayofyear(col(\"ts_paris\"))\n).withColumn(\n    \"is_weekend\",\n    when((col(\"dow\") == 1) | (col(\"dow\") == 7), 1).otherwise(0)\n)\n\n# Cyclical time features\nweather_mapped = weather_mapped.withColumn(\n    \"hour_sin\",\n    sin(2 * 3.14159 * col(\"hour\") / 24.0)\n).withColumn(\n    \"hour_cos\", \n    cos(2 * 3.14159 * col(\"hour\") / 24.0)\n).withColumn(\n    \"month_sin\",\n    sin(2 * 3.14159 * col(\"month\") / 12.0)\n).withColumn(\n    \"month_cos\",\n    cos(2 * 3.14159 * col(\"month\") / 12.0)\n)\n\n# Seasonal features\nweather_mapped = weather_mapped.withColumn(\n    \"season\",\n    when((col(\"month\") >= 3) & (col(\"month\") <= 5), \"spring\")\n    .when((col(\"month\") >= 6) & (col(\"month\") <= 8), \"summer\")\n    .when((col(\"month\") >= 9) & (col(\"month\") <= 11), \"autumn\")\n    .otherwise(\"winter\")\n).withColumn(\n    \"is_daylight\",\n    when((col(\"hour\") >= 6) & (col(\"hour\") <= 20), 1).otherwise(0)\n)\n\n# Weather severity based on temperature\nweather_mapped = weather_mapped.withColumn(\n    \"weather_severity\",\n    when(col(\"temp_c\") < 0, \"cold\")\n    .when(col(\"temp_c\") > 30, \"hot\") \n    .otherwise(\"moderate\")\n)\n\nprint(\"Final weather_mapped schema:\")\nweather_mapped.printSchema()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Creating weather features...\nFinal weather_mapped schema:\nroot\n |-- city_norm: string (nullable = true)\n |-- city_name: string (nullable = true)\n |-- geoname_id: string (nullable = true)\n |-- date: long (nullable = true)\n |-- temperature_2m: double (nullable = true)\n |-- temperature_2m_previous_day1: double (nullable = true)\n |-- temperature_2m_previous_day2: double (nullable = true)\n |-- temperature_2m_previous_day3: double (nullable = true)\n |-- temperature_2m_previous_day4: double (nullable = true)\n |-- temperature_2m_previous_day5: double (nullable = true)\n |-- country_code: string (nullable = true)\n |-- population: long (nullable = true)\n |-- city_timezone: string (nullable = true)\n |-- lat: double (nullable = true)\n |-- lon: double (nullable = true)\n |-- region_clean: string (nullable = true)\n |-- region_canon: string (nullable = true)\n |-- ts_utc: timestamp (nullable = true)\n |-- ts_paris: timestamp (nullable = true)\n |-- temp_c: double (nullable = true)\n |-- date_paris: date (nullable = true)\n |-- hour: integer (nullable = true)\n |-- dow: integer (nullable = true)\n |-- month: integer (nullable = true)\n |-- doy: integer (nullable = true)\n |-- is_weekend: integer (nullable = false)\n |-- hour_sin: double (nullable = true)\n |-- hour_cos: double (nullable = true)\n |-- month_sin: double (nullable = true)\n |-- month_cos: double (nullable = true)\n |-- season: string (nullable = false)\n |-- is_daylight: integer (nullable = false)\n |-- weather_severity: string (nullable = false)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Regional Aggregation\n# ============================================\n\nprint(\"Starting regional aggregation...\")\n\n# Aggregation based on available columns\nagg_exprs = []\n\n# Average temperature\nif 'temp_c' in weather_mapped.columns:\n    agg_exprs.append(avg(\"temp_c\").alias(\"temp_c\"))\n\n# Also include the previous day temperatures if available\nfor col_name in available_numeric_columns:\n    if col_name != 'temperature_2m':  # temperature_2m already handled via temp_c\n        agg_exprs.append(avg(col_name).alias(col_name))\n\n# Add other features\nagg_exprs.extend([\n    first(\"season\").alias(\"season\"),\n    first(\"is_daylight\").alias(\"is_daylight\"),\n    first(\"hour\").alias(\"hour\"),\n    first(\"dow\").alias(\"dow\"),\n    first(\"month\").alias(\"month\"),\n    first(\"is_weekend\").alias(\"is_weekend\"),\n    first(\"weather_severity\").alias(\"weather_severity\")\n])\n\nweather_aggregated = weather_mapped.groupBy(\"region_clean\", \"ts_utc\").agg(*agg_exprs)\n\nprint(f\"After aggregation: {weather_aggregated.count()} rows\")\n\n# Add back cyclical features after aggregation\nweather_aggregated = weather_aggregated.withColumn(\n    \"hour_sin\",\n    sin(2 * 3.14159 * col(\"hour\") / 24.0)\n).withColumn(\n    \"hour_cos\", \n    cos(2 * 3.14159 * col(\"hour\") / 24.0)\n).withColumn(\n    \"month_sin\",\n    sin(2 * 3.14159 * col(\"month\") / 12.0)\n).withColumn(\n    \"month_cos\",\n    cos(2 * 3.14159 * col(\"month\") / 12.0)\n)\n\n# Final data quality checks\nweather_final = weather_aggregated.filter(\n    (col(\"temp_c\") > -50) & (col(\"temp_c\") < 60)\n)\n\nprint(f\"Final transformed row count: {weather_final.count()}\")\nprint(\"Sample of final weather data:\")\nweather_final.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Starting regional aggregation...\nAfter aggregation: 50880 rows\nFinal transformed row count: 50880\nSample of final weather data:\n+------------+-------------------+------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+------+-----------+----+---+-----+----------+----------------+-------------------+-------------------+--------------------+-------------------+\n|region_clean|             ts_utc|            temp_c|temperature_2m_previous_day1|temperature_2m_previous_day2|temperature_2m_previous_day3|temperature_2m_previous_day4|temperature_2m_previous_day5|season|is_daylight|hour|dow|month|is_weekend|weather_severity|           hour_sin|           hour_cos|           month_sin|          month_cos|\n+------------+-------------------+------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+------+-----------+----+---+-----+----------+----------------+-------------------+-------------------+--------------------+-------------------+\n|      Alsace|2025-07-23 13:00:00|24.631500244140625|          25.131500244140625|          26.229249954223633|          25.979249954223633|          23.504249572753906|          21.804999351501465|summer|          1|  15|  4|    7|         0|        moderate|-0.7071044357184857|-0.7071091266468295|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-23 17:00:00| 24.68150043487549|           24.95650005340576|           26.42924976348877|          25.904250144958496|           19.35425090789795|          19.979999542236328|summer|          1|  19|  4|    7|         0|        moderate|-0.9659269137132054|0.25881498674628944|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-24 14:00:00|22.881500244140625|           21.70650005340576|          24.554250717163086|           24.17924976348877|          21.679250717163086|          20.255000114440918|summer|          1|  16|  5|    7|         0|        moderate|-0.8660236347191557|-0.5000030640984338|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-25 02:00:00| 16.95650005340576|           17.33150005340576|          14.879250049591064|          17.529250144958496|          16.154250144958496|          15.304999828338623|summer|          0|   4|  6|    7|         0|        moderate| 0.8660249615191342| 0.5000007660251953|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-25 07:00:00|17.506500244140625|           17.45650005340576|           17.80424976348877|           18.07925033569336|          17.554250717163086|          16.755000114440918|summer|          1|   9|  6|    7|         0|        moderate|   0.70710818846365|-0.7071053739066443|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-25 17:00:00|  23.7814998626709|          23.681499481201172|          23.654250144958496|           21.45425033569336|          22.379249572753906|          22.005000114440918|summer|          1|  19|  6|    7|         0|        moderate|-0.9659269137132054|0.25881498674628944|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-26 08:00:00|  19.9064998626709|           20.43150043487549|           18.57925033569336|          19.029250144958496|          18.279250144958496|           20.93000030517578|summer|          1|  10|  7|    7|         1|        moderate| 0.5000019150622543|-0.8660242981199074|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-27 01:00:00| 17.98150062561035|           18.48150062561035|          17.754250526428223|          17.529250144958496|          17.379250526428223|          14.505000114440918|summer|          0|   3|  1|    7|         1|        moderate| 0.7071063120935575| 0.7071072502792264|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-27 18:00:00| 18.70650005340576|          18.156500816345215|          18.754249572753906|          19.354249954223633|          18.254249572753906|          22.630000114440918|summer|          1|  20|  1|    7|         1|        moderate|-0.8660276151007966|0.49999616986815654|-0.49999731890873617|-0.8660269517076681|\n|      Alsace|2025-07-27 19:00:00|18.381500244140625|          17.531500816345215|           17.67924976348877|          18.529250144958496|          17.679250717163086|           21.55500030517578|summer|          0|  21|  1|    7|         1|        moderate|-0.7071100648287638| 0.7071034975290827|-0.49999731890873617|-0.8660269517076681|\n+------------+-------------------+------------------+----------------------------+----------------------------+----------------------------+----------------------------+----------------------------+------+-----------+----+---+-----+----------+----------------+-------------------+-------------------+--------------------+-------------------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Write to Silver Bucket\n# ============================================\n\nweather_dynamic_frame = DynamicFrame.fromDF(\n    weather_final, \n    glueContext, \n    \"weather_transformed\"\n)\n\nsink = glueContext.getSink(\n    path=f\"s3://{source_bucket}/{processed_folder_name}/weather_transformed/\",\n    connection_type=\"s3\",\n    updateBehavior=\"UPDATE_IN_DATABASE\",\n    partitionKeys=[],\n    compression=\"snappy\",\n    enableUpdateCatalog=True,\n    transformation_ctx=\"sink\",\n)\n\nsink.setCatalogInfo(\n    catalogDatabase=glue_database,\n    catalogTableName=\"silver_weather_data\"\n)\n\nsink.setFormat(\"glueparquet\")\nsink.writeFrame(weather_dynamic_frame)\n\nprint(f\"SUCCESS: Written transformed weather data to silver layer\")\n\n# Commit the job\njob.commit()",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}