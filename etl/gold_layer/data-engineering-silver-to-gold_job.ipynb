{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%timeout 60",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.7 \nCurrent timeout is None minutes.\ntimeout has been set to 20 minutes.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "%%configure\n{\n    \"--job-bookmark-option\": \"job-bookmark-enable\"\n}",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "The following configurations have been updated: {'--job-bookmark-option': 'job-bookmark-enable'}\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import *\nfrom awsglue.dynamicframe import DynamicFrame\nfrom awsglue.utils import getResolvedOptions\nfrom awsglue.job import Job\nimport hashlib\n\n# Initialize all the variables needed\nsource_bucket = \"data-engineering-project-8433-3658-8863\"\nsilver_folder = \"silver_data\"\ngold_folder = \"gold_data\"\n\n# Set up catalog parameters\nglue_database = \"data-engineering-project-glue-database\"\n\n# Set up the spark contexts, glue contexts and initialize job\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)\n\ntry:\n    args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n    JOB_NAME = args['JOB_NAME']\nexcept:\n    JOB_NAME = \"notebook-job-gold-schema-enhanced\"\n\njob.init(JOB_NAME, args if 'args' in locals() else {})\n\n# Set additional Spark configurations to handle legacy date formats\nspark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"LEGACY\")\nspark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"LEGACY\")\nspark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"LEGACY\") \nspark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Trying to create a Glue session for the kernel.\nSession Type: glueetl\nTimeout: 20\nSession ID: f1e6c7d9-6b07-4c75-8af3-f03c23ddb966\nApplying the following default arguments:\n--glue_kernel_version 1.0.7\n--enable-glue-datacatalog true\n--job-bookmark-option job-bookmark-enable\nWaiting for session f1e6c7d9-6b07-4c75-8af3-f03c23ddb966 to get into ready status...\nSession f1e6c7d9-6b07-4c75-8af3-f03c23ddb966 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Helper Functions\n# ============================================\n\ndef generate_id(*values):\n    \"\"\"Generate consistent hash ID from values\"\"\"\n    input_str = \"_\".join([str(v) for v in values if v is not None])\n    return hashlib.sha256(input_str.encode()).hexdigest()\n\n# Register UDF\ngenerate_id_udf = udf(generate_id, StringType())",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 2,
			"outputs": [
				{
					"name": "stdout",
					"text": "\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Read Silver Layer Tables\n# ============================================\n\nprint(\"Reading Silver Layer tables...\")\n\n# Read all silver tables\nelectricity_silver = glueContext.create_dynamic_frame.from_catalog(\n    database=glue_database,\n    table_name=\"silver_electricity_data\",\n    additional_options = {\n        \"parquet.datetime.rebase.mode\": \"LEGACY\",\n        \"parquet.int96.rebase.mode\": \"LEGACY\"\n    }\n).toDF()\n\nweather_silver = glueContext.create_dynamic_frame.from_catalog(\n    database=glue_database, \n    table_name=\"silver_weather_data\",\n    additional_options = {\n        \"parquet.datetime.rebase.mode\": \"LEGACY\", \n        \"parquet.int96.rebase.mode\": \"LEGACY\"\n    }\n).toDF()\n\nholiday_silver = glueContext.create_dynamic_frame.from_catalog(\n    database=glue_database,\n    table_name=\"silver_holiday_data\", \n    additional_options = {\n        \"parquet.datetime.rebase.mode\": \"LEGACY\",\n        \"parquet.int96.rebase.mode\": \"LEGACY\"\n    }\n).toDF()\n\nregion_silver = glueContext.create_dynamic_frame.from_catalog(\n    database=glue_database,\n    table_name=\"silver_region_mapping\",\n    additional_options = {\n        \"parquet.datetime.rebase.mode\": \"LEGACY\",\n        \"parquet.int96.rebase.mode\": \"LEGACY\"\n    }\n).toDF()\n\nprint(\"Silver tables loaded:\")\nprint(f\"Electricity: {electricity_silver.count()} rows\")\nprint(f\"Weather: {weather_silver.count()} rows\") \nprint(f\"Holiday: {holiday_silver.count()} rows\")\nprint(f\"Region: {region_silver.count()} rows\")\n\n# Show schemas and available columns\nprint(\"Electricity columns:\", electricity_silver.columns)\nprint(\"Weather columns:\", weather_silver.columns)\nprint(\"Holiday columns:\", holiday_silver.columns)\nprint(\"Region columns:\", region_silver.columns)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "Reading Silver Layer tables...\nSilver tables loaded:\nElectricity: 349020 rows\nWeather: 50880 rows\nHoliday: 28 rows\nRegion: 326 rows\nElectricity columns: ['region_clean', 'ts_hour_utc', 'consommation_mw', 'solaire_mw', 'eolien_mw', 'hydraulique_mw', 'nucleaire_mw', 'thermique_mw', 'pompage_mw', 'bioenergies_mw', 'stockage_batterie_mw', 'destockage_batterie_mw', 'eolien_terrestre_mw', 'eolien_offshore_mw', 'ech_physiques_mw', 'ts_paris', 'hour', 'dow', 'week', 'month', 'doy', 'is_weekend', 'hour_sin', 'hour_cos', 'consommation_lag24', 'consommation_lag168', 'consommation_roll168']\nWeather columns: ['region_clean', 'ts_utc', 'temp_c', 'temperature_2m_previous_day1', 'temperature_2m_previous_day2', 'temperature_2m_previous_day3', 'temperature_2m_previous_day4', 'temperature_2m_previous_day5', 'season', 'is_daylight', 'hour', 'dow', 'month', 'is_weekend', 'weather_severity', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos']\nHoliday columns: ['date_paris', 'ts_utc', 'ts_paris', 'holiday_name', 'holiday_type', 'holiday_category', 'location', 'is_national_holiday', 'is_regional_holiday', 'is_observance', 'is_bridge_day', 'is_weekend_adjacent', 'is_easter_related', 'holiday_season', 'year', 'month', 'day', 'dow', 'doy', 'week', 'quarter']\nRegion columns: ['city_norm', 'city_raw', 'region_clean', 'region_canon', 'cap_lat', 'cap_lon']\n/opt/amazon/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py:127: UserWarning: DataFrame constructor is internal. Do not directly use it.\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Check distinct regions in each table\nprint(\"Distinct regions in silver_electricity:\")\nelectricity_silver.select(\"region_clean\").distinct().show()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 4,
			"outputs": [
				{
					"name": "stdout",
					"text": "Distinct regions in silver_electricity:\n+--------------------+\n|        region_clean|\n+--------------------+\n|           Grand-Est|\n|     Hauts-de-France|\n|  Nouvelle-Aquitaine|\n|Bourgogne-Franche...|\n|           Occitanie|\n|Auvergne-Rhône-Alpes|\n| Centre-Val de Loire|\n|Provence-Alpes-Co...|\n|           Normandie|\n|            Bretagne|\n|       Ile-de-France|\n|    Pays-de-la-Loire|\n+--------------------+\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# SILVER LAYER REGION HARMONIZATION - FIXED VERSION\n# ============================================\n\nprint(\"Starting region name harmonization across Gold Layer tables...\")\n\n# Define helper functions at module level (not inside UDF)\nimport unicodedata\nimport re\n\ndef normalize_text(text):\n    \"\"\"Normalize text by removing accents and special characters\"\"\"\n    if text is None:\n        return \"\"\n    # Remove accents and convert to lowercase\n    text = unicodedata.normalize('NFKD', str(text))\n    text = ''.join(c for c in text if not unicodedata.combining(c))\n    # Replace special characters with spaces and normalize\n    for ch in [\"'\", \"’\", \"-\", \"_\", \".\", \",\", \"(\", \")\", \"/\", \"\\\\\"]:\n        text = text.replace(ch, \" \")\n    # Normalize whitespace and convert to lowercase\n    return \" \".join(text.split()).lower()\n\ndef harmonize_region_name(region_name):\n    \"\"\"Harmonize all region names to consistent modern French administrative names\"\"\"\n    if region_name is None:\n        return None\n    \n    normalized_input = normalize_text(region_name)\n    \n    # Target standard names (exactly as they should appear in final output)\n    TARGET_STANDARDS = {\n        \"auvergne rhone alpes\": \"Auvergne-Rhône-Alpes\",\n        \"bourgogne franche comte\": \"Bourgogne-Franche-Comté\", \n        \"bretagne\": \"Bretagne\",\n        \"centre val de loire\": \"Centre-Val de Loire\",\n        \"grand est\": \"Grand-Est\",\n        \"hauts de france\": \"Hauts-de-France\",\n        \"ile de france\": \"Île-de-France\",\n        \"normandie\": \"Normandie\",\n        \"nouvelle aquitaine\": \"Nouvelle-Aquitaine\",\n        \"occitanie\": \"Occitanie\",\n        \"pays de la loire\": \"Pays-de-la-Loire\",\n        \"provence alpes cote d azur\": \"Provence-Alpes-Côte d'Azur\",\n        \"corse\": \"Corse\"\n    }\n    \n    # Comprehensive mapping from all possible variants to target standards\n    COMPREHENSIVE_MAPPING = {\n        # Old English names → Target standards\n        \"alsace\": \"Grand-Est\",\n        \"aquitaine\": \"Nouvelle-Aquitaine\", \n        \"auvergne\": \"Auvergne-Rhône-Alpes\",\n        \"basse normandie\": \"Normandie\",\n        \"brittany\": \"Bretagne\",\n        \"burgundy\": \"Bourgogne-Franche-Comté\",\n        \"centre\": \"Centre-Val de Loire\", \n        \"champagne ardenne\": \"Grand-Est\",\n        \"corsica\": \"Corse\",\n        \"franche comte\": \"Bourgogne-Franche-Comté\",\n        \"haute normandie\": \"Normandie\",\n        \"languedoc roussillon\": \"Occitanie\", \n        \"limousin\": \"Nouvelle-Aquitaine\",\n        \"lorraine\": \"Grand-Est\",\n        \"midi pyrenees\": \"Occitanie\",\n        \"nord pas de calais\": \"Hauts-de-France\",\n        \"picardy\": \"Hauts-de-France\",\n        \"picardie\": \"Hauts-de-France\",\n        \"poitou charentes\": \"Nouvelle-Aquitaine\", \n        \"rhone alpes\": \"Auvergne-Rhône-Alpes\",\n        \n        # Common variants and abbreviations\n        \"paca\": \"Provence-Alpes-Côte d'Azur\",\n        \"provence alpes\": \"Provence-Alpes-Côte d'Azur\",\n        \"provence alpes cote d azur\": \"Provence-Alpes-Côte d'Azur\",\n        \"provence alpes cote d'azur\": \"Provence-Alpes-Côte d'Azur\",\n        \"cote d azur\": \"Provence-Alpes-Côte d'Azur\",\n        \n        # Handle different spellings of Île-de-France\n        \"ile de france\": \"Île-de-France\",\n        \"île de france\": \"Île-de-France\", \n        \n        # Handle Pays-de-la-Loire variations\n        \"pays de la loire\": \"Pays-de-la-Loire\",\n        \"pays de loire\": \"Pays-de-la-Loire\",\n        \n        # Add the target standards themselves\n        **TARGET_STANDARDS\n    }\n    \n    # Exact match in comprehensive mapping\n    if normalized_input in COMPREHENSIVE_MAPPING:\n        return COMPREHENSIVE_MAPPING[normalized_input]\n    \n    # Try to find partial matches\n    for source, target in COMPREHENSIVE_MAPPING.items():\n        if source in normalized_input or normalized_input in source:\n            return target\n    \n    # Final fallback: return original name\n    return region_name\n\n# Register UDF\nharmonize_region_name_udf = udf(harmonize_region_name, StringType())\n\n# Target standards for final validation\nTARGET_STANDARDS_LIST = [\n    \"Auvergne-Rhône-Alpes\", \"Bourgogne-Franche-Comté\", \"Bretagne\", \"Centre-Val de Loire\",\n    \"Grand-Est\", \"Hauts-de-France\", \"Île-de-France\", \"Normandie\", \"Nouvelle-Aquitaine\",\n    \"Occitanie\", \"Pays-de-la-Loire\", \"Provence-Alpes-Côte d'Azur\", \"Corse\"\n]\n\ndef final_standardize_region(region_name):\n    \"\"\"Ensure region name exactly matches one of the target standards\"\"\"\n    if region_name in TARGET_STANDARDS_LIST:\n        return region_name\n    \n    # Simple normalization for comparison\n    if region_name is None:\n        return None\n    \n    region_simple = region_name.lower().replace(\"-\", \" \").replace(\"'\", \" \").replace(\"é\", \"e\").replace(\"ô\", \"o\")\n    region_simple = \" \".join(region_simple.split())\n    \n    for target in TARGET_STANDARDS_LIST:\n        target_simple = target.lower().replace(\"-\", \" \").replace(\"'\", \" \").replace(\"é\", \"e\").replace(\"ô\", \"o\")\n        target_simple = \" \".join(target_simple.split())\n        \n        if region_simple == target_simple:\n            return target\n            \n        # Check for close matches\n        if (region_simple in target_simple or \n            target_simple in region_simple or \n            region_simple.replace(\" \", \"\") == target_simple.replace(\" \", \"\")):\n            return target\n    \n    return region_name\n\nfinal_standardize_region_udf = udf(final_standardize_region, StringType())\n\n# Apply harmonization to all tables\nprint(\"Applying region harmonization...\")\n\n# Apply first level of harmonization\nelectricity_silver_harmonized = electricity_silver.withColumn(\n    \"region_clean_temp\", \n    harmonize_region_name_udf(col(\"region_clean\"))\n)\n\nweather_silver_harmonized = weather_silver.withColumn(\n    \"region_clean_temp\",\n    harmonize_region_name_udf(col(\"region_clean\"))\n)\n\n\n\n# Apply final standardization\nelectricity_silver_final = electricity_silver_harmonized.withColumn(\n    \"region_clean_standard\", final_standardize_region_udf(col(\"region_clean_temp\"))\n).drop(\"region_clean\", \"region_clean_temp\").withColumnRenamed(\"region_clean_standard\", \"region_clean\")\n\nweather_silver_final = weather_silver_harmonized.withColumn(\n    \"region_clean_standard\", final_standardize_region_udf(col(\"region_clean_temp\"))\n).drop(\"region_clean\", \"region_clean_temp\").withColumnRenamed(\"region_clean_standard\", \"region_clean\")\n\n\n# Show final results\nprint(\"FINAL HARMONIZED REGIONS:\")\nprint(\"Electricity regions:\")\nelectricity_silver_final.select(\"region_clean\").distinct().orderBy(\"region_clean\").show(truncate=False)\n\nprint(\"Weather regions:\")\nweather_silver_final.select(\"region_clean\").distinct().orderBy(\"region_clean\").show(truncate=False)\n\n\n\n# Verify all regions match\nelectricity_regions = [row.region_clean for row in electricity_silver_final.select(\"region_clean\").distinct().collect()]\nweather_regions = [row.region_clean for row in weather_silver_final.select(\"region_clean\").distinct().collect()]\n\n\nprint(f\"Electricity regions count: {len(electricity_regions)}\")\nprint(f\"Weather regions count: {len(weather_regions)}\")\n\n\n# Check if all regions are in the target standards\nall_regions = set(electricity_regions + weather_regions)\nnon_standard_regions = [r for r in all_regions if r not in TARGET_STANDARDS_LIST]\n\nif non_standard_regions:\n    print(f\"WARNING: Non-standard regions found: {non_standard_regions}\")\nelse:\n    print(\"SUCCESS: All regions are standardized!\")\n\n# Replace the original tables\nelectricity_silver = electricity_silver_final\nweather_silver = weather_silver_final  \n\n\nprint(\"Region harmonization completed successfully!\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 6,
			"outputs": [
				{
					"name": "stdout",
					"text": "Starting region name harmonization across Gold Layer tables...\nApplying region harmonization...\nFINAL HARMONIZED REGIONS:\nElectricity regions:\n+--------------------------+\n|region_clean              |\n+--------------------------+\n|Auvergne-Rhône-Alpes      |\n|Bourgogne-Franche-Comté   |\n|Bretagne                  |\n|Centre-Val de Loire       |\n|Grand-Est                 |\n|Hauts-de-France           |\n|Normandie                 |\n|Nouvelle-Aquitaine        |\n|Occitanie                 |\n|Pays-de-la-Loire          |\n|Provence-Alpes-Côte d'Azur|\n|Île-de-France             |\n+--------------------------+\n\nWeather regions:\n+--------------------------+\n|region_clean              |\n+--------------------------+\n|Auvergne-Rhône-Alpes      |\n|Bourgogne-Franche-Comté   |\n|Bretagne                  |\n|Centre-Val de Loire       |\n|Grand-Est                 |\n|Hauts-de-France           |\n|Normandie                 |\n|Nouvelle-Aquitaine        |\n|Occitanie                 |\n|Pays-de-la-Loire          |\n|Provence-Alpes-Côte d'Azur|\n|Île-de-France             |\n+--------------------------+\n\nElectricity regions count: 12\nWeather regions count: 12\nSUCCESS: All regions are standardized!\nRegion harmonization completed successfully!\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Data Preparation & Cleaning - ROBUST VERSION\n# ============================================\n\nprint(\"Preparing and cleaning data with robust column handling...\")\n\n# First, let's see what timestamp columns we actually have\nprint(\"Available timestamp columns in electricity:\", [col for col in electricity_silver.columns if 'ts' in col or 'time' in col])\nprint(\"Available timestamp columns in weather:\", [col for col in weather_silver.columns if 'ts' in col or 'time' in col])\n\n# Create base timestamp columns for electricity\nif 'ts_hour_utc' in electricity_silver.columns:\n    electricity_silver = electricity_silver.withColumn(\"ts_utc\", col(\"ts_hour_utc\"))\nelif 'ts_utc' in electricity_silver.columns:\n    electricity_silver = electricity_silver.withColumn(\"ts_utc\", col(\"ts_utc\"))\nelse:\n    # If no UTC timestamp, create from Paris time or use current as fallback\n    if 'ts_paris' in electricity_silver.columns:\n        electricity_silver = electricity_silver.withColumn(\"ts_utc\", to_utc_timestamp(col(\"ts_paris\"), \"Europe/Paris\"))\n    else:\n        electricity_silver = electricity_silver.withColumn(\"ts_utc\", current_timestamp())\n\n# Create Paris time for electricity\nif 'ts_paris' in electricity_silver.columns:\n    electricity_silver = electricity_silver.withColumn(\"ts_paris\", col(\"ts_paris\"))\nelse:\n    electricity_silver = electricity_silver.withColumn(\"ts_paris\", from_utc_timestamp(col(\"ts_utc\"), \"Europe/Paris\"))\n\n# Create base timestamp columns for weather\nif 'ts_utc' in weather_silver.columns:\n    weather_silver = weather_silver.withColumn(\"ts_utc\", col(\"ts_utc\"))\nelse:\n    # Try to find any timestamp column\n    timestamp_cols = [col for col in weather_silver.columns if 'date' in col or 'time' in col or 'ts' in col]\n    if timestamp_cols:\n        weather_silver = weather_silver.withColumn(\"ts_utc\", to_timestamp(col(timestamp_cols[0])))\n    else:\n        weather_silver = weather_silver.withColumn(\"ts_utc\", current_timestamp())\n\n# Create Paris time for weather\nweather_silver = weather_silver.withColumn(\"ts_paris\", from_utc_timestamp(col(\"ts_utc\"), \"Europe/Paris\"))\n\nprint(\"After timestamp processing:\")\nprint(\"Electricity timestamp columns:\", [col for col in electricity_silver.columns if 'ts' in col])\nprint(\"Weather timestamp columns:\", [col for col in weather_silver.columns if 'ts' in col])\n\n# Add time components using the actual available timestamp columns\ntime_components = [\n    (\"year\", year(col(\"ts_paris\"))),\n    (\"month\", month(col(\"ts_paris\"))),\n    (\"day\", dayofmonth(col(\"ts_paris\"))),\n    (\"hour\", hour(col(\"ts_paris\"))),\n    (\"dow\", dayofweek(col(\"ts_paris\"))),\n    (\"doy\", dayofyear(col(\"ts_paris\"))),\n    (\"week\", weekofyear(col(\"ts_paris\"))),\n    (\"quarter\", quarter(col(\"ts_paris\"))),\n    (\"is_weekend\", when((dayofweek(col(\"ts_paris\")).isin([1, 7])), 1).otherwise(0))\n]\n\n# Add time components only if they don't exist\nfor col_name, expr in time_components:\n    if col_name not in electricity_silver.columns:\n        electricity_silver = electricity_silver.withColumn(col_name, expr)\n    if col_name not in weather_silver.columns:\n        weather_silver = weather_silver.withColumn(col_name, expr)\n\nprint(\"After adding time components:\")\nprint(\"Electricity columns:\", [col for col in electricity_silver.columns if col in [name for name, _ in time_components]])\nprint(\"Weather columns:\", [col for col in weather_silver.columns if col in [name for name, _ in time_components]])",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "Preparing and cleaning data with robust column handling...\nAvailable timestamp columns in electricity: ['ts_hour_utc', 'ts_paris']\nAvailable timestamp columns in weather: ['ts_utc']\nAfter timestamp processing:\nElectricity timestamp columns: ['ts_hour_utc', 'ts_paris', 'ts_utc']\nWeather timestamp columns: ['ts_utc', 'ts_paris']\nAfter adding time components:\nElectricity columns: ['hour', 'dow', 'week', 'month', 'doy', 'is_weekend', 'year', 'day', 'quarter']\nWeather columns: ['hour', 'dow', 'month', 'is_weekend', 'year', 'day', 'doy', 'week', 'quarter']\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Create Dimension Tables\n# ============================================\n\nprint(\"Creating Dimension Tables...\")\n\n# Dim_Time - Comprehensive time dimension\nprint(\"Creating dim_time...\")\ndim_time = electricity_silver.select(\n    \"ts_utc\", \"ts_paris\", \"hour\", \"dow\", \"month\", \"year\", \"doy\", \"week\", \"quarter\", \"is_weekend\"\n).distinct().filter(col(\"ts_utc\").isNotNull()).withColumn(\n    \"time_id\",\n    generate_id_udf(col(\"ts_utc\"))\n).withColumn(\n    \"time_hour_key\",\n    date_format(col(\"ts_utc\"), \"yyyyMMddHH\")\n).withColumn(\n    \"month_name\",\n    date_format(col(\"ts_paris\"), \"MMMM\")\n).withColumn(\n    \"day_name\", \n    date_format(col(\"ts_paris\"), \"EEEE\")\n).withColumn(\n    \"is_workday\",\n    when((col(\"is_weekend\") == 1) | (col(\"dow\").isin([1, 7])), 0).otherwise(1)\n).withColumn(\n    \"season\",\n    when((col(\"month\") >= 3) & (col(\"month\") <= 5), \"spring\")\n    .when((col(\"month\") >= 6) & (col(\"month\") <= 8), \"summer\")\n    .when((col(\"month\") >= 9) & (col(\"month\") <= 11), \"autumn\")\n    .otherwise(\"winter\")\n).select(\n    \"time_id\", \"time_hour_key\", \"ts_utc\", \"ts_paris\", \"hour\", \n    \"dow\", \"day_name\", \"month\", \"month_name\", \"quarter\", \"year\",\n    \"doy\", \"week\", \"is_weekend\", \"is_workday\", \"season\"\n).distinct().orderBy(\"ts_utc\")\n\nprint(f\"dim_time: {dim_time.count()} rows\")\ndim_time.show(10)\n\n# Dim_Region - Region dimension with geographic info\n# Dim_Region - Region dimension with harmonized names\nprint(\"Creating dim_region...\")\n\n# Zuerst die korrekten modernen Regionalnamen aus electricity_silver nehmen\nmodern_regions_from_electricity = electricity_silver.select(\n    \"region_clean\"\n).distinct().filter(col(\"region_clean\").isNotNull())\n\n# Alte region mapping Daten für geo-Koordinaten verwenden, aber Namen modernisieren\nregion_mapping_modernized = modern_regions_from_electricity.withColumn(\n    \"region_canon\", \n    col(\"region_clean\")  # Verwende die modernen Namen direkt\n).withColumn(\n    \"country\",\n    lit(\"France\")\n)\n\n# Geo-Koordinaten für die modernen Regionen hinzufügen (falls verfügbar)\nREGION_CAPITALS_MODERN = {\n    \"Auvergne-Rhône-Alpes\": (45.7640, 4.8357),\n    \"Bourgogne-Franche-Comté\": (47.3220, 5.0415),\n    \"Bretagne\": (48.1173, -1.6778),\n    \"Centre-Val de Loire\": (47.9029, 1.9093),\n    \"Grand-Est\": (48.5734, 7.7521),\n    \"Hauts-de-France\": (50.6292, 3.0573),\n    \"Île-de-France\": (48.8566, 2.3522),\n    \"Normandie\": (49.4432, 1.0993),\n    \"Nouvelle-Aquitaine\": (44.8378, -0.5792),\n    \"Occitanie\": (43.6047, 1.4442),\n    \"Pays-de-la-Loire\": (47.2184, -1.5536),\n    \"Provence-Alpes-Cote d'Azur\": (43.2965, 5.3698),\n    \"Corse\": (41.9192, 8.7386),\n}\n\n# Create capitals DataFrame with modern names\ncapitals_data_modern = [(region, coords[0], coords[1]) for region, coords in REGION_CAPITALS_MODERN.items()]\ncapitals_df_modern = spark.createDataFrame(capitals_data_modern, [\"region_clean\", \"cap_lat\", \"cap_lon\"])\n\n# Final dim_region mit harmonisierten Namen\ndim_region = region_mapping_modernized.join(\n    capitals_df_modern, \n    on=\"region_clean\", \n    how=\"left\"\n).withColumn(\n    \"region_id\",\n    generate_id_udf(col(\"region_clean\"))\n).select(\n    \"region_id\", \"region_clean\", \"region_canon\", \"country\", \"cap_lat\", \"cap_lon\"\n).distinct().orderBy(\"region_clean\")\n\nprint(\"Modernized dim_region:\")\ndim_region.show(truncate=False)\n\n# Dim_Weather_Type - Weather type dimension using ACTUAL weather columns\nprint(\"Creating dim_weather_type...\")\n\n# Check what weather columns actually exist\navailable_weather_cols = weather_silver.columns\nprint(\"Available weather columns for categorization:\", available_weather_cols)\n\n# Use only existing columns for categorization\nweather_categories_data = []\n\nif 'temp_c' in available_weather_cols:\n    # Create categories based on actual temperature data\n    weather_categories = weather_silver.select(\"temp_c\").distinct().filter(col(\"temp_c\").isNotNull()).withColumn(\n        \"temperature_category\",\n        when(col(\"temp_c\") < 0, \"very_cold\")\n        .when(col(\"temp_c\") < 10, \"cold\")\n        .when(col(\"temp_c\") < 20, \"cool\")\n        .when(col(\"temp_c\") < 30, \"warm\")\n        .otherwise(\"hot\")\n    )\nelse:\n    # Fallback: create basic categories\n    categories_data = [(\"cold\",), (\"cool\",), (\"warm\",), (\"hot\",)]\n    weather_categories = spark.createDataFrame(categories_data, [\"temperature_category\"])\n\n# Add precipitation categories if available\nif 'precip_mm' in available_weather_cols:\n    weather_categories = weather_categories.crossJoin(\n        weather_silver.select(\"precip_mm\").distinct().filter(col(\"precip_mm\").isNotNull()).withColumn(\n            \"precipitation_category\",\n            when(col(\"precip_mm\") > 5, \"heavy_rain\")\n            .when(col(\"precip_mm\") > 1, \"light_rain\")\n            .when(col(\"precip_mm\") > 0, \"drizzle\")\n            .otherwise(\"dry\")\n        )\n    )\nelse:\n    precip_data = [(\"dry\",), (\"drizzle\",), (\"light_rain\",), (\"heavy_rain\",)]\n    precip_categories = spark.createDataFrame(precip_data, [\"precipitation_category\"])\n    weather_categories = weather_categories.crossJoin(precip_categories)\n\n# Add wind categories if available  \nif 'wind_kph' in available_weather_cols:\n    weather_categories = weather_categories.crossJoin(\n        weather_silver.select(\"wind_kph\").distinct().filter(col(\"wind_kph\").isNotNull()).withColumn(\n            \"wind_category\",\n            when(col(\"wind_kph\") > 30, \"strong_wind\")\n            .when(col(\"wind_kph\") > 15, \"moderate_wind\")\n            .otherwise(\"light_wind\")\n        )\n    )\nelse:\n    wind_data = [(\"light_wind\",), (\"moderate_wind\",), (\"strong_wind\",)]\n    wind_categories = spark.createDataFrame(wind_data, [\"wind_category\"])\n    weather_categories = weather_categories.crossJoin(wind_categories)\n\n# Create final weather type dimension\ndim_weather_type = weather_categories.withColumn(\n    \"weather_type_id\",\n    generate_id_udf(col(\"temperature_category\"), col(\"precipitation_category\"), col(\"wind_category\"))\n).select(\n    \"weather_type_id\", \"temperature_category\", \"precipitation_category\", \"wind_category\"\n).distinct()\n\nprint(f\"dim_weather_type: {dim_weather_type.count()} rows\")\ndim_weather_type.show(10)\n\n# Dim_Holiday - Holiday dimension\nprint(\"Creating dim_holiday...\")\ndim_holiday = holiday_silver.filter(col(\"date_paris\").isNotNull()).withColumn(\n    \"holiday_id\",\n    generate_id_udf(col(\"date_paris\"), col(\"holiday_name\"))\n).select(\n    \"holiday_id\", \"date_paris\", \"holiday_name\", \"holiday_type\", \n    \"holiday_category\", \"is_national_holiday\", \"is_regional_holiday\",\n    \"is_observance\", \"is_bridge_day\", \"is_weekend_adjacent\", \n    \"is_easter_related\", \"holiday_season\"\n).distinct().orderBy(\"date_paris\")\n\nprint(f\"dim_holiday: {dim_holiday.count()} rows\")\ndim_holiday.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Creating Dimension Tables...\nCreating dim_time...\ndim_time: 29084 rows\n+--------------------+-------------+-------------------+-------------------+----+---+---------+-----+----------+-------+----+---+----+----------+----------+------+\n|             time_id|time_hour_key|             ts_utc|           ts_paris|hour|dow| day_name|month|month_name|quarter|year|doy|week|is_weekend|is_workday|season|\n+--------------------+-------------+-------------------+-------------------+----+---+---------+-----+----------+-------+----+---+----+----------+----------+------+\n|dc7e591acb59fe3c2...|   2024123123|2024-12-31 23:00:00|2025-01-01 00:00:00|   0|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|5878f17df5620bfd9...|   2024123123|2024-12-31 23:15:00|2025-01-01 00:15:00|   0|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|94b323c591b63cc9b...|   2024123123|2024-12-31 23:30:00|2025-01-01 00:30:00|   0|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|672b415fac3df5024...|   2024123123|2024-12-31 23:45:00|2025-01-01 00:45:00|   0|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|a938adc37b86eee57...|   2025010100|2025-01-01 00:00:00|2025-01-01 01:00:00|   1|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|17884375543a47184...|   2025010100|2025-01-01 00:15:00|2025-01-01 01:15:00|   1|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|ec451ec951f821f07...|   2025010100|2025-01-01 00:30:00|2025-01-01 01:30:00|   1|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|23deb2a1f9bb5887e...|   2025010100|2025-01-01 00:45:00|2025-01-01 01:45:00|   1|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|b8e72deedde676dfe...|   2025010101|2025-01-01 01:00:00|2025-01-01 02:00:00|   2|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n|89057889524a495aa...|   2025010101|2025-01-01 01:15:00|2025-01-01 02:15:00|   2|  4|Wednesday|    1|   January|      1|2025|  1|   1|         0|         1|winter|\n+--------------------+-------------+-------------------+-------------------+----+---+---------+-----+----------+-------+----+---+----+----------+----------+------+\nonly showing top 10 rows\n\nCreating dim_region...\nModernized dim_region:\n+----------------------------------------------------------------+--------------------------+--------------------------+-------+-------+-------+\n|region_id                                                       |region_clean              |region_canon              |country|cap_lat|cap_lon|\n+----------------------------------------------------------------+--------------------------+--------------------------+-------+-------+-------+\n|bfe813ca66d5f2188c3b70c8d92bc3d5cf02e31a7d05dbcf3a0eb577c55d4921|Auvergne-Rhône-Alpes      |Auvergne-Rhône-Alpes      |France |45.764 |4.8357 |\n|c622b7a59ae88175f4b310201385502d84dc4de914dbd6b895a64bc7164ac319|Bourgogne-Franche-Comté   |Bourgogne-Franche-Comté   |France |47.322 |5.0415 |\n|ebcb0dee945cb488536c882812d1618fedf79678dcc5b49c036103cf59c03e88|Bretagne                  |Bretagne                  |France |48.1173|-1.6778|\n|d005afa1aa489c4d00e4189f0756f96b26e2bc8a097ab6a60f0638798a843807|Centre-Val de Loire       |Centre-Val de Loire       |France |47.9029|1.9093 |\n|64bea2192e1f6ce8feaa837ccdbd8125f6d77e12654cabcf72510a6cb13aad8c|Grand-Est                 |Grand-Est                 |France |48.5734|7.7521 |\n|68eb0f6e8fc7dfa5b4993a61d6aeab0d755ced6747a2bd75b25350d80d3bd41a|Hauts-de-France           |Hauts-de-France           |France |50.6292|3.0573 |\n|e425f3b7a68b10f2eee77fa5951f6ffce3a5fdd557c5305b8b4e923b8b7897ce|Normandie                 |Normandie                 |France |49.4432|1.0993 |\n|fff55b91eb14ebb7fc4da00999d870203c4b3edf320e93603671654b9d2487d5|Nouvelle-Aquitaine        |Nouvelle-Aquitaine        |France |44.8378|-0.5792|\n|8116a7590abbe3c6e345a8e7da45062899a318bde38458a2d9d73139139b200b|Occitanie                 |Occitanie                 |France |43.6047|1.4442 |\n|6f79e0e3c82c9c472b3ffe7c3f16f760534f3bfcac02fdeeb0743e9ca23284a5|Pays-de-la-Loire          |Pays-de-la-Loire          |France |47.2184|-1.5536|\n|fc9a9a297a94bb68122e6bb98f8d1a2c82263aff5620bcd122e99927714a34b6|Provence-Alpes-Côte d'Azur|Provence-Alpes-Côte d'Azur|France |null   |null   |\n|8733be9706f611cbfaeeb86a24b87f84b25801aa92ffcbc774fe1c7b77ca7317|Île-de-France             |Île-de-France             |France |48.8566|2.3522 |\n+----------------------------------------------------------------+--------------------------+--------------------------+-------+-------+-------+\n\nCreating dim_weather_type...\nAvailable weather columns for categorization: ['ts_utc', 'temp_c', 'temperature_2m_previous_day1', 'temperature_2m_previous_day2', 'temperature_2m_previous_day3', 'temperature_2m_previous_day4', 'temperature_2m_previous_day5', 'season', 'is_daylight', 'hour', 'dow', 'month', 'is_weekend', 'weather_severity', 'hour_sin', 'hour_cos', 'month_sin', 'month_cos', 'region_clean', 'ts_paris', 'year', 'day', 'doy', 'week', 'quarter']\ndim_weather_type: 48 rows\n+--------------------+--------------------+----------------------+-------------+\n|     weather_type_id|temperature_category|precipitation_category|wind_category|\n+--------------------+--------------------+----------------------+-------------+\n|95c2b57dbd54d905b...|                warm|               drizzle|  strong_wind|\n|9e0ec11f3af321db0...|                cold|               drizzle|  strong_wind|\n|d49b50b600cc1d16b...|                 hot|               drizzle|  strong_wind|\n|55268d7a61a1eea1b...|                cool|               drizzle|  strong_wind|\n|b956269eccbcbf4ff...|                cool|            light_rain|  strong_wind|\n|5875964f42b421cfa...|                 hot|            light_rain|  strong_wind|\n|873f2085f797c785c...|                warm|            light_rain|  strong_wind|\n|ec9f44c09585c5368...|                cold|            light_rain|  strong_wind|\n|42e96a15c3239e3a0...|                cold|                   dry|  strong_wind|\n|a618544b23acd226e...|                cool|                   dry|  strong_wind|\n+--------------------+--------------------+----------------------+-------------+\nonly showing top 10 rows\n\nCreating dim_holiday...\ndim_holiday: 28 rows\n+--------------------+----------+--------------------+-------------+----------------+-------------------+-------------------+-------------+-------------+-------------------+-----------------+---------------+\n|          holiday_id|date_paris|        holiday_name| holiday_type|holiday_category|is_national_holiday|is_regional_holiday|is_observance|is_bridge_day|is_weekend_adjacent|is_easter_related| holiday_season|\n+--------------------+----------+--------------------+-------------+----------------+-------------------+-------------------+-------------+-------------+-------------------+-----------------+---------------+\n|11ed43065064a2cd7...|2025-01-01|      New Year's Day|     National|        national|                  1|                  0|            0|            0|                  0|                0|       new_year|\n|baabbfca9d7921106...|2025-03-20|       March Equinox|     Seasonal|           other|                  0|                  0|            0|            0|                  0|                0|regular_holiday|\n|71d30aa21c5f93b00...|2025-03-30|Daylight Saving T...| Clock change|           other|                  0|                  0|            0|            0|                  1|                0|regular_holiday|\n|af77ecdd8b4abc878...|2025-04-18|         Good Friday|Local holiday|           other|                  0|                  0|            0|            0|                  0|                1|regular_holiday|\n|ae59f39588c6fbe00...|2025-04-20|       Easter Sunday|   Observance|      observance|                  0|                  0|            1|            0|                  1|                1|regular_holiday|\n|cb4a59cf2c8521e76...|2025-04-21|       Easter Monday|     National|        national|                  1|                  0|            0|            1|                  1|                1|regular_holiday|\n|cb4a59cf2c8521e76...|2025-04-21|       Easter Monday|     National|        national|                  1|                  0|            0|            0|                  0|                1|regular_holiday|\n|86f7531e5714925a8...|2025-05-01| Labor Day / May Day|     National|        national|                  1|                  0|            0|            0|                  0|                0|regular_holiday|\n|21c616dae5b73c2f9...|2025-05-08|    WWII Victory Day|     National|        national|                  1|                  0|            0|            0|                  0|                0|regular_holiday|\n|38806aa82eddc9104...|2025-05-25|        Mother's Day|   Observance|      observance|                  0|                  0|            1|            0|                  1|                0|regular_holiday|\n+--------------------+----------+--------------------+-------------+----------------+-------------------+-------------------+-------------+-------------+-------------------+-----------------+---------------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Apply harmonization to dim_region\ndim_region_harmonized = dim_region.withColumn(\n    \"region_clean_temp\", \n    harmonize_region_name_udf(col(\"region_clean\"))\n)\n\n# Apply final standardization\ndim_region_final = dim_region_harmonized.withColumn(\n    \"region_clean_standard\", final_standardize_region_udf(col(\"region_clean_temp\"))\n).drop(\"region_clean\", \"region_clean_temp\").withColumnRenamed(\"region_clean_standard\", \"region_clean\")\n\n# Show final results\nprint(\"FINAL HARMONIZED REGIONS:\")\n\nprint(\"Dim_region regions:\")\ndim_region_final.select(\"region_clean\").distinct().orderBy(\"region_clean\").show(truncate=False)\n\n# Verify all regions match\ndim_regions = [row.region_clean for row in dim_region_final.select(\"region_clean\").distinct().collect()]\nprint(f\"Dim_region regions count: {len(dim_regions)}\")\n\ndim_region = dim_region_final\nprint(\"Region harmonization completed successfully!\")",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "FINAL HARMONIZED REGIONS:\nDim_region regions:\n+--------------------------+\n|region_clean              |\n+--------------------------+\n|Auvergne-Rhône-Alpes      |\n|Bourgogne-Franche-Comté   |\n|Bretagne                  |\n|Centre-Val de Loire       |\n|Grand-Est                 |\n|Hauts-de-France           |\n|Normandie                 |\n|Nouvelle-Aquitaine        |\n|Occitanie                 |\n|Pays-de-la-Loire          |\n|Provence-Alpes-Côte d'Azur|\n|Île-de-France             |\n+--------------------------+\n\nDim_region regions count: 12\nRegion harmonization completed successfully!\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Create Core Fact Tables\n# ============================================\n\nprint(\"Creating Core Fact Tables...\")\n\n# Fact_Electricity_Hourly - Main electricity consumption facts\nprint(\"Creating fact_electricity_hourly...\")\n\n# Prepare electricity data with foreign keys - MIT HARMONISIERTEN NAMEN\nelectricity_with_keys = electricity_silver.alias(\"e\").join(\n    dim_time.alias(\"t\"), \n    col(\"e.ts_utc\") == col(\"t.ts_utc\"),\n    \"inner\"\n).join(\n    dim_region.alias(\"r\"),\n    col(\"e.region_clean\") == col(\"r.region_clean\"),  # Jetzt sollten die Namen übereinstimmen\n    \"inner\"\n).join(\n    dim_holiday.alias(\"h\"),\n    to_date(col(\"e.ts_paris\")) == col(\"h.date_paris\"),\n    \"left\"\n)\n\n# Define energy columns with safe defaults - only use existing columns\nenergy_columns_mapping = {\n    \"consommation_mw\": \"consumption_mw\",\n    \"solaire_mw\": \"solar_production_mw\",\n    \"eolien_mw\": \"wind_production_mw\",\n    \"hydraulique_mw\": \"hydro_production_mw\", \n    \"nucleaire_mw\": \"nuclear_production_mw\",\n    \"thermique_mw\": \"thermal_production_mw\",\n    \"bioenergies_mw\": \"bioenergy_production_mw\"\n}\n\n# Build select expressions\nelectricity_select_exprs = [\n    generate_id_udf(col(\"e.ts_utc\"), col(\"e.region_clean\")).alias(\"electricity_record_id\"),\n    col(\"t.time_id\"),\n    col(\"r.region_id\"),\n    col(\"h.holiday_id\")\n]\n\n# Add energy columns with coalesce for null values - only if they exist\nfor source_col, target_col in energy_columns_mapping.items():\n    if source_col in electricity_silver.columns:\n        electricity_select_exprs.append(coalesce(col(f\"e.{source_col}\"), lit(0.0)).alias(target_col))\n    else:\n        print(f\"Warning: Column {source_col} not found in electricity data, skipping\")\n        electricity_select_exprs.append(lit(0.0).alias(target_col))\n\n# Add calculated fields and metadata\nelectricity_select_exprs.extend([\n    col(\"e.ts_utc\"),\n    col(\"e.region_clean\")\n])\n\n# Add renewable ratio calculation if we have the required columns\nif \"consommation_mw\" in electricity_silver.columns:\n    # Build renewable sum expression manually\n    renewable_sum = lit(0.0)\n    \n    # Add each renewable source if it exists\n    for source_col in [\"solaire_mw\", \"eolien_mw\", \"hydraulique_mw\"]:\n        if source_col in electricity_silver.columns:\n            renewable_sum = renewable_sum + coalesce(col(f\"e.{source_col}\"), lit(0.0))\n    \n    renewable_expr = renewable_sum / greatest(col(\"e.consommation_mw\"), lit(1.0))\n    electricity_select_exprs.append(renewable_expr.alias(\"renewable_ratio\"))\nelse:\n    electricity_select_exprs.append(lit(0.0).alias(\"renewable_ratio\"))\n\nfact_electricity_hourly = electricity_with_keys.select(*electricity_select_exprs).filter(\n    col(\"consumption_mw\") > 0\n)\n\nprint(f\"fact_electricity_hourly: {fact_electricity_hourly.count()} rows\")\nfact_electricity_hourly.show(10)\n\n# Fact_Weather_Hourly - Main weather observations\nprint(\"Creating fact_weather_hourly...\")\n\n# Prepare weather data with foreign keys - MIT HARMONISIERTEN NAMEN\nweather_with_keys = weather_silver.alias(\"w\").join(\n    dim_time.alias(\"t\"),\n    col(\"w.ts_utc\") == col(\"t.ts_utc\"),\n    \"inner\"\n).join(\n    dim_region.alias(\"r\"),\n    col(\"w.region_clean\") == col(\"r.region_clean\"),  # Jetzt sollten die Namen übereinstimmen\n    \"inner\"\n)\n\n# Define weather columns with safe defaults - only use existing columns\nweather_columns_mapping = {\n    \"temp_c\": \"temperature_c\",\n    \"humidity\": \"humidity_percent\", \n    \"wind_kph\": \"wind_speed_kph\",\n    \"cloud\": \"cloud_cover_percent\",\n    \"pressure_mb\": \"pressure_mb\",\n    \"precip_mm\": \"precipitation_mm\"\n}\n\n# Build select expressions\nweather_select_exprs = [\n    generate_id_udf(col(\"w.ts_utc\"), col(\"w.region_clean\")).alias(\"weather_record_id\"),\n    col(\"t.time_id\"),\n    col(\"r.region_id\")\n]\n\n# Add weather_type_id based on available categories\nif all(col in weather_silver.columns for col in ['temp_c', 'precip_mm', 'wind_kph']):\n    # Join with weather type dimension if we have all required columns\n    weather_with_keys = weather_with_keys.join(\n        dim_weather_type.alias(\"wt\"),\n        ( \n            (col(\"w.temp_c\") < 0) & (col(\"wt.temperature_category\") == \"very_cold\") |\n            (col(\"w.temp_c\") < 10) & (col(\"wt.temperature_category\") == \"cold\") |\n            (col(\"w.temp_c\") < 20) & (col(\"wt.temperature_category\") == \"cool\") |\n            (col(\"w.temp_c\") < 30) & (col(\"wt.temperature_category\") == \"warm\") |\n            (col(\"wt.temperature_category\") == \"hot\")\n        ) &\n        (\n            (col(\"w.precip_mm\") == 0) & (col(\"wt.precipitation_category\") == \"dry\") |\n            (col(\"w.precip_mm\") > 0) & (col(\"w.precip_mm\") <= 1) & (col(\"wt.precipitation_category\") == \"drizzle\") |\n            (col(\"w.precip_mm\") > 1) & (col(\"w.precip_mm\") <= 5) & (col(\"wt.precipitation_category\") == \"light_rain\") |\n            (col(\"w.precip_mm\") > 5) & (col(\"wt.precipitation_category\") == \"heavy_rain\")\n        ) &\n        (\n            (col(\"w.wind_kph\") <= 15) & (col(\"wt.wind_category\") == \"light_wind\") |\n            (col(\"w.wind_kph\") > 15) & (col(\"w.wind_kph\") <= 30) & (col(\"wt.wind_category\") == \"moderate_wind\") |\n            (col(\"w.wind_kph\") > 30) & (col(\"wt.wind_category\") == \"strong_wind\")\n        ),\n        \"left\"\n    )\n    weather_select_exprs.append(col(\"wt.weather_type_id\"))\nelse:\n    weather_select_exprs.append(lit(\"default\").alias(\"weather_type_id\"))\n\n# Add weather columns with coalesce for null values - only if they exist\nfor source_col, target_col in weather_columns_mapping.items():\n    if source_col in weather_silver.columns:\n        weather_select_exprs.append(coalesce(col(f\"w.{source_col}\"), lit(0.0)).alias(target_col))\n    else:\n        print(f\"Warning: Column {source_col} not found in weather data, skipping\")\n        weather_select_exprs.append(lit(0.0).alias(target_col))\n\nweather_select_exprs.extend([\n    col(\"w.ts_utc\"),\n    col(\"w.region_clean\")\n])\n\nfact_weather_hourly = weather_with_keys.select(*weather_select_exprs).filter(\n    col(\"temperature_c\").between(-50, 50)  # Basic data quality check\n)\n\nprint(f\"fact_weather_hourly: {fact_weather_hourly.count()} rows\")\nfact_weather_hourly.show(10)",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 11,
			"outputs": [
				{
					"name": "stdout",
					"text": "Creating Core Fact Tables...\nCreating fact_electricity_hourly...\nfact_electricity_hourly: 350632 rows\n+---------------------+--------------------+--------------------+--------------------+--------------+-------------------+------------------+-------------------+---------------------+---------------------+-----------------------+-------------------+------------+-------------------+\n|electricity_record_id|             time_id|           region_id|          holiday_id|consumption_mw|solar_production_mw|wind_production_mw|hydro_production_mw|nuclear_production_mw|thermal_production_mw|bioenergy_production_mw|             ts_utc|region_clean|    renewable_ratio|\n+---------------------+--------------------+--------------------+--------------------+--------------+-------------------+------------------+-------------------+---------------------+---------------------+-----------------------+-------------------+------------+-------------------+\n| f1350396665964d78...|dc7e591acb59fe3c2...|64bea2192e1f6ce8f...|11ed43065064a2cd7...|        5649.0|                0.0|            1279.0|              727.0|               9730.0|                152.0|                   78.0|2024-12-31 23:00:00|   Grand-Est| 0.3551070986015224|\n| 0eb713b20e3a85d96...|0ca7173cce8a33d99...|64bea2192e1f6ce8f...|11ed43065064a2cd7...|        5134.0|                0.0|            1852.0|              602.0|               7710.0|                136.0|                   78.0|2025-01-01 05:15:00|   Grand-Est|0.47798987144526683|\n| f0295f3a9e4d7a07b...|3b1329581085c7e40...|64bea2192e1f6ce8f...|11ed43065064a2cd7...|        5168.0|               24.0|            2124.0|              782.0|               7659.0|                157.0|                   78.0|2025-01-01 07:15:00|   Grand-Est| 0.5669504643962848|\n| 2854986ad34e4467f...|d83be38e29fa44161...|64bea2192e1f6ce8f...|11ed43065064a2cd7...|        5080.0|               66.0|            2546.0|              713.0|               7686.0|                156.0|                   78.0|2025-01-01 08:15:00|   Grand-Est| 0.6545275590551181|\n| 24ad1677446531e65...|68160ec28a481d3cc...|64bea2192e1f6ce8f...|11ed43065064a2cd7...|        5120.0|              453.0|            2838.0|              586.0|               7526.0|                155.0|                   78.0|2025-01-01 11:00:00|   Grand-Est|       0.7572265625|\n| 8762bd4f63ca057f6...|babd6cd18e24961f6...|64bea2192e1f6ce8f...|11ed43065064a2cd7...|        5149.0|              437.0|            2896.0|              653.0|               7560.0|                154.0|                   78.0|2025-01-01 11:45:00|   Grand-Est| 0.7741308992037289|\n| 493a55c06ac615290...|99560a4606d0b2804...|64bea2192e1f6ce8f...|11ed43065064a2cd7...|        5377.0|               15.0|            3568.0|             1136.0|               7711.0|                156.0|                   78.0|2025-01-01 17:00:00|   Grand-Est| 0.8776269295145992|\n| 843a91b8e9d835ffc...|cd3cc7ef8850cf58b...|64bea2192e1f6ce8f...|11ed43065064a2cd7...|        5126.0|                0.0|            4085.0|              840.0|               7556.0|                154.0|                   78.0|2025-01-01 20:45:00|   Grand-Est| 0.9607881388997269|\n| 7f34e5ce339d0b4d4...|77bbfef10ec9796ac...|64bea2192e1f6ce8f...|                null|        4772.0|                0.0|            3250.0|              613.0|               8821.0|                156.0|                   78.0|2025-01-02 05:00:00|   Grand-Est| 0.8095138306789607|\n| 213e93af028263c2b...|a93fdb08220e0ba17...|64bea2192e1f6ce8f...|                null|        6154.0|                0.0|             243.0|             1106.0|              10627.0|               1426.0|                   78.0|2025-01-02 17:45:00|   Grand-Est|0.21920701982450438|\n+---------------------+--------------------+--------------------+--------------------+--------------+-------------------+------------------+-------------------+---------------------+---------------------+-----------------------+-------------------+------------+-------------------+\nonly showing top 10 rows\n\nCreating fact_weather_hourly...\nWarning: Column humidity not found in weather data, skipping\nWarning: Column wind_kph not found in weather data, skipping\nWarning: Column cloud not found in weather data, skipping\nWarning: Column pressure_mb not found in weather data, skipping\nWarning: Column precip_mm not found in weather data, skipping\nfact_weather_hourly: 47960 rows\n+--------------------+--------------------+--------------------+---------------+------------------+----------------+--------------+-------------------+-----------+----------------+-------------------+------------+\n|   weather_record_id|             time_id|           region_id|weather_type_id|     temperature_c|humidity_percent|wind_speed_kph|cloud_cover_percent|pressure_mb|precipitation_mm|             ts_utc|region_clean|\n+--------------------+--------------------+--------------------+---------------+------------------+----------------+--------------+-------------------+-----------+----------------+-------------------+------------+\n|ebac0a170d6ff6f28...|7626bfee197c1681b...|e425f3b7a68b10f2e...|        default|11.405000305175781|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-29 18:00:00|   Normandie|\n|ebac0a170d6ff6f28...|7626bfee197c1681b...|e425f3b7a68b10f2e...|        default| 12.54384994506836|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-29 18:00:00|   Normandie|\n|5a0f1550a24a81034...|06930ee6818f1eb98...|e425f3b7a68b10f2e...|        default|12.903849887847901|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-28 20:00:00|   Normandie|\n|5a0f1550a24a81034...|06930ee6818f1eb98...|e425f3b7a68b10f2e...|        default|11.535000038146972|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-28 20:00:00|   Normandie|\n|6b945dcaa32ab071d...|fc12b1009c02fcfcf...|e425f3b7a68b10f2e...|        default|10.842949867248535|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-26 22:00:00|   Normandie|\n|6b945dcaa32ab071d...|fc12b1009c02fcfcf...|e425f3b7a68b10f2e...|        default|10.843000602722167|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-26 22:00:00|   Normandie|\n|4fe604e12f3c9a902...|768d77a6b722924c6...|e425f3b7a68b10f2e...|        default| 10.33794994354248|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-26 21:00:00|   Normandie|\n|4fe604e12f3c9a902...|768d77a6b722924c6...|e425f3b7a68b10f2e...|        default|10.593000411987305|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-26 21:00:00|   Normandie|\n|152ab605c12dc759f...|c516919526fcbc305...|e425f3b7a68b10f2e...|        default| 8.533000183105468|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-26 05:00:00|   Normandie|\n|152ab605c12dc759f...|c516919526fcbc305...|e425f3b7a68b10f2e...|        default| 9.177949905395508|             0.0|           0.0|                0.0|        0.0|             0.0|2025-10-26 05:00:00|   Normandie|\n+--------------------+--------------------+--------------------+---------------+------------------+----------------+--------------+-------------------+-----------+----------------+-------------------+------------+\nonly showing top 10 rows\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# ============================================\n# Write Gold Layer to S3\n# ============================================\n\nprint(\"Writing Gold Layer tables to S3...\")\n\n# Function to write table with error handling\ndef write_gold_table(df, table_name, partition_cols=None):\n    try:\n        dynamic_frame = DynamicFrame.fromDF(df, glueContext, table_name)\n        \n        sink = glueContext.getSink(\n            path=f\"s3://{source_bucket}/{gold_folder}/{table_name}/\",\n            connection_type=\"s3\",\n            updateBehavior=\"UPDATE_IN_DATABASE\",\n            partitionKeys=partition_cols or [],\n            compression=\"snappy\",\n            enableUpdateCatalog=True,\n            transformation_ctx=f\"sink_{table_name}\",\n        )\n        \n        sink.setCatalogInfo(\n            catalogDatabase=glue_database,\n            catalogTableName=table_name\n        )\n        \n        sink.setFormat(\"glueparquet\")\n        sink.writeFrame(dynamic_frame)\n        print(f\"✓ Successfully written {table_name}\")\n        \n    except Exception as e:\n        print(f\"✗ Error writing {table_name}: {str(e)}\")\n\n# Write dimension tables\nwrite_gold_table(dim_time, \"dim_time\")\nwrite_gold_table(dim_region, \"dim_region\") \nwrite_gold_table(dim_weather_type, \"dim_weather_type\")\nwrite_gold_table(dim_holiday, \"dim_holiday\")\n\n# Write fact tables\nwrite_gold_table(fact_electricity_hourly, \"fact_electricity_hourly\", [\"region_clean\"])\nwrite_gold_table(fact_weather_hourly, \"fact_weather_hourly\", [\"region_clean\"])\n\nprint(\"Gold Layer transformation completed successfully!\")\nprint(f\"All tables written to: s3://{source_bucket}/{gold_folder}/\")\n\n# Show final counts\nprint(\"\\nFinal table counts:\")\nprint(f\"dim_time: {dim_time.count()}\")\nprint(f\"dim_region: {dim_region.count()}\")\nprint(f\"dim_weather_type: {dim_weather_type.count()}\")\nprint(f\"dim_holiday: {dim_holiday.count()}\")\nprint(f\"fact_electricity_hourly: {fact_electricity_hourly.count()}\")\nprint(f\"fact_weather_hourly: {fact_weather_hourly.count()}\")\n\n# Commit the job\njob.commit()",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 8,
			"outputs": [
				{
					"name": "stdout",
					"text": "Writing Gold Layer tables to S3...\n✓ Successfully written dim_time\n✓ Successfully written dim_region\n✓ Successfully written dim_weather_type\n✓ Successfully written dim_holiday\n✓ Successfully written fact_electricity_hourly\n✓ Successfully written fact_weather_hourly\nGold Layer transformation completed successfully!\nAll tables written to: s3://data-engineering-project2-432801802552/gold_data/\n\nFinal table counts:\ndim_time: 29084\ndim_region: 22\ndim_weather_type: 48\ndim_holiday: 28\nfact_electricity_hourly: 29219\nfact_weather_hourly: 47960\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}